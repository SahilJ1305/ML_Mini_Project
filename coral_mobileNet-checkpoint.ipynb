{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80250e61-e6ce-48ce-b2a0-799f7cd2ae7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pycocotools\n",
      "  Downloading pycocotools-2.0.8-cp311-cp311-win_amd64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in c:\\users\\rugved\\anaconda3\\lib\\site-packages (from pycocotools) (3.8.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\rugved\\anaconda3\\lib\\site-packages (from pycocotools) (1.26.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\rugved\\anaconda3\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\rugved\\anaconda3\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\rugved\\anaconda3\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\rugved\\anaconda3\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rugved\\anaconda3\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\rugved\\anaconda3\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\rugved\\anaconda3\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\rugved\\anaconda3\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rugved\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.16.0)\n",
      "Downloading pycocotools-2.0.8-cp311-cp311-win_amd64.whl (85 kB)\n",
      "   ---------------------------------------- 0.0/85.3 kB ? eta -:--:--\n",
      "   -------------- ------------------------- 30.7/85.3 kB 1.3 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 30.7/85.3 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 85.3/85.3 kB 682.4 kB/s eta 0:00:00\n",
      "Installing collected packages: pycocotools\n",
      "Successfully installed pycocotools-2.0.8\n"
     ]
    }
   ],
   "source": [
    "!pip install pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1f334af-9a32-41c1-9526-6485943623f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection import RetinaNet\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "from torchvision.models import mobilenet_v3_small\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from torchvision.ops import box_iou\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69790388-4812-4b35-9e03-23a8e3b82420",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CoralDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform = transform\n",
    "        self.class_map = {'Healthy Coral': 2, 'Dead Coral': 1, 'Bleached Coral': 0}\n",
    "        self.image_files = list(self.root_dir.glob('*.jpg')) + list(self.root_dir.glob('*.png'))\n",
    "        self.annotation_cache = {}\n",
    "\n",
    "    def __len__(self): return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        xml_path = self.root_dir / f'{img_path.stem}.xml'\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # Load cached or parse XML\n",
    "        if img_path in self.annotation_cache:\n",
    "            boxes, labels = self.annotation_cache[img_path]\n",
    "        else:\n",
    "            tree = ET.parse(xml_path)\n",
    "            root = tree.getroot()\n",
    "            size = root.find('size')\n",
    "            width = int(size.find('width').text)\n",
    "            height = int(size.find('height').text)\n",
    "            \n",
    "            boxes, labels = [], []\n",
    "            for obj in root.iter('object'):\n",
    "                class_name = obj.find('name').text\n",
    "                if class_name not in self.class_map: continue\n",
    "                \n",
    "                bndbox = obj.find('bndbox')\n",
    "                xmin = float(bndbox.find('xmin').text) / width\n",
    "                ymin = float(bndbox.find('ymin').text) / height\n",
    "                xmax = float(bndbox.find('xmax').text) / width\n",
    "                ymax = float(bndbox.find('ymax').text) / height\n",
    "                \n",
    "                boxes.append([xmin, ymin, xmax, ymax])\n",
    "                labels.append(self.class_map[class_name])\n",
    "            \n",
    "            self.annotation_cache[img_path] = (boxes, labels)\n",
    "\n",
    "        # Convert to tensors\n",
    "        # print(f\"Labels for image {img_path.name}: {labels}\")\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "        # print(f\"Shape of labels tensor for {img_path.name}: {labels.shape}\") # Add this line\n",
    "        \n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'image_id': torch.tensor([idx]),\n",
    "            'area': (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]),\n",
    "            'iscrowd': torch.zeros(len(boxes), dtype=torch.int64)\n",
    "        }\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "c97ac889-bd37-4ded-ae12-bd0e2b1e244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoralRetinaNetMobile:\n",
    "    def __init__(self, num_classes=3, pretrained=True):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.class_names = ['Bleached', 'Dead', 'Healthy']\n",
    "        self.checkpoint_dir = Path('checkpoints')\n",
    "        self.checkpoint_dir.mkdir(exist_ok=True)\n",
    "        self.best_score = 0.0\n",
    "        self.best_combined = 0.0  # Add this\n",
    "        self.best_dead_precision = 0.0  # Add this\n",
    "        self.train_losses = []\n",
    "        self.val_losses = [] \n",
    "        \n",
    "        # Model setup\n",
    "        backbone = mobilenet_v3_small(weights='DEFAULT' if pretrained else None).features\n",
    "        backbone.out_channels = 576\n",
    "        \n",
    "        anchor_generator = AnchorGenerator(\n",
    "            sizes=((16, 32, 64, 128),),\n",
    "            aspect_ratios=((0.5, 1.0, 2.0),) * 4\n",
    "        )\n",
    "        \n",
    "        self.model = RetinaNet(\n",
    "            backbone,\n",
    "            num_classes=num_classes,\n",
    "            anchor_generator=anchor_generator,\n",
    "            box_score_thresh=0.25\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Checkpoint loading\n",
    "        if (self.checkpoint_dir / 'last.pt').exists():\n",
    "            self.load_checkpoint()\n",
    "        else:\n",
    "            self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "            self.scaler = torch.amp.GradScaler()\n",
    "            self.start_epoch = 0\n",
    "            self.best_map = 0.0  # Track mAP instead of loss\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        checkpoint = torch.load(self.checkpoint_dir / 'last.pt', map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoint['model'])\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=1e-5, weight_decay=1e-4)  #lr=1e-5 wd=0\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        self.start_epoch = checkpoint['epoch'] + 1\n",
    "        self.best_map = checkpoint.get('best_map', 0.0)\n",
    "        self.scaler = torch.amp.GradScaler()\n",
    "        print(f\"Resuming training from epoch {self.start_epoch}\")\n",
    "\n",
    "    def get_transform(self, train=True):\n",
    "        return transforms.Compose([\n",
    "            transforms.ToImage(),\n",
    "            transforms.ConvertImageDtype(torch.float32),\n",
    "            transforms.Lambda(lambda img: img / 255.0),\n",
    "            transforms.Resize((320, 320), antialias=True),\n",
    "            transforms.RandomHorizontalFlip(p=0.3) if train else transforms.Identity(),\n",
    "            # transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "            # transforms.RandomRotation(degrees=(-10, 10)),\n",
    "            # transforms.RandomVerticalFlip(p=0.1),\n",
    "        ])\n",
    "\n",
    "    def create_loaders(self, data_root):\n",
    "        train_loader = DataLoader(\n",
    "            CoralDataset(Path(data_root)/'train', self.get_transform(True)),\n",
    "            batch_size=16,\n",
    "            shuffle=True,\n",
    "            num_workers=0,  # Required for Windows stability\n",
    "            pin_memory=True,\n",
    "            collate_fn=lambda x: tuple(zip(*x))\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            CoralDataset(Path(data_root)/'valid', self.get_transform(False)),\n",
    "            batch_size=32,\n",
    "            num_workers=0,\n",
    "            pin_memory=True,\n",
    "            collate_fn=lambda x: tuple(zip(*x))\n",
    "        )\n",
    "        return train_loader, val_loader\n",
    "\n",
    "    def train(self, train_loader, epochs=80, val_loader=None, early_stop_patience=2):\n",
    "        self.model.train()\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        early_stop_counter = 0\n",
    "    # Ensure self.best_score is defined in __init__ (e.g., self.best_score = 0.0)\n",
    "\n",
    "        for epoch in range(self.start_epoch, epochs):\n",
    "            print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "            epoch_loss = 0.0\n",
    "\n",
    "        # Training phase\n",
    "            for i, (images, targets) in enumerate(train_loader):\n",
    "                losses = torch.tensor(0.0, device=self.device) # Initialize losses here\n",
    "                try:\n",
    "                    images = [img.to(self.device) for img in images]\n",
    "                    targets = [{k: v.to(self.device) for k, v in t.items()} for t in targets]\n",
    "                    # print(\"First target in the batch:\", targets[0])\n",
    "                    # print(\"Image Shapes:\", [img.shape for img in images])\n",
    "                    # print(\"Target Shapes:\", [t['boxes'].shape for t in targets], [t['labels'].shape for t in targets]) # Add this line\n",
    "                    \n",
    "                    with torch.amp.autocast(device_type='cuda', enabled=self.device.type == 'cuda'):\n",
    "                        loss_dict = self.model(images, targets)\n",
    "                        losses = loss_dict[\"classification\"] + loss_dict[\"bbox_regression\"]\n",
    "                        #print(\"Loss Dictionary:\", loss_dict)\n",
    "                        # losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "                        # Handle loss calculation safely\n",
    "                        if isinstance(loss_dict, dict):\n",
    "                            losses = sum(torch.mean(loss) for loss in loss_dict.values())\n",
    "                        elif isinstance(loss_dict, list):\n",
    "                            losses = sum(\n",
    "                            torch.mean(sum(item.values())) if isinstance(item, dict) else torch.mean(item)\n",
    "                            for item in loss_dict\n",
    "                            )\n",
    "                        else:\n",
    "                            raise ValueError(f\"Unexpected loss type: {type(loss_dict)}\")\n",
    "\n",
    "                    self.optimizer.zero_grad()\n",
    "                    self.scaler.scale(losses).backward()\n",
    "                    self.scaler.step(self.optimizer)\n",
    "                    self.scaler.update()\n",
    "                    epoch_loss += losses.item()\n",
    "\n",
    "\n",
    "                # Log batch loss every 50 batches\n",
    "                    if (i + 1) % 50 == 0:\n",
    "                        print(f\"Batch {i+1}/{len(train_loader)} - Loss: {losses.item():.4f}\")\n",
    "                except RuntimeError as e:\n",
    "                    print(f\"Error in batch {i+1}: {e}\")\n",
    "                    print(f\"Image shapes: {[img.shape for img in images]}\")\n",
    "                    print(f\"Target keys: {[list(t.keys()) for t in targets]}\")\n",
    "                    raise e\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in batch {i+1}/{len(train_loader)}: {str(e)}\")\n",
    "                    print(f\"Image shapes: {[img.shape for img in images]}\")\n",
    "                    print(f\"Target keys: {[list(t.keys()) for t in targets]}\")\n",
    "                    raise e\n",
    "\n",
    "            avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "            self.train_losses.append(avg_epoch_loss)\n",
    "            print(f\"Epoch {epoch+1} - Train Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "        # Validation phase\n",
    "            if val_loader:\n",
    "                metrics = self.validate(val_loader)  # Assume it returns a dict with key 'f1'\n",
    "                current_map = metrics['map']  # Will always exist\n",
    "                current_f1 = metrics['f1']  # From cls_metrics\n",
    "                dead_coral_recall = metrics['per_class_metrics']['Dead']['recall']\n",
    "                \n",
    "            # Use F1 score for early stopping\n",
    "                # 1. Primary: Mean Average Precision (mAP)\n",
    "                if metrics['map'] > self.best_map:\n",
    "                    self.best_map = metrics['map']\n",
    "                    self.save_checkpoint(epoch, 'best_map.pt')\n",
    "    \n",
    "    # 2. Secondary: Combined Detection+Classification Score\n",
    "                combined_score = (0.7 * metrics['map'] + \n",
    "                        0.3 * metrics['f1'])\n",
    "                if combined_score > self.best_combined:\n",
    "                    self.best_combined = combined_score\n",
    "                    self.save_checkpoint(epoch, 'best_combined.pt')\n",
    "    \n",
    "    # 3. Class-Specific: Critical Class Precision (e.g., \"Dead\" coral)\n",
    "                # dead_precision = metrics['confusion_matrix'][1,1] / metrics['confusion_matrix'][:,1].sum()\n",
    "                # if dead_precision > self.best_dead_precision:\n",
    "                #     self.best_dead_precision = dead_precision\n",
    "                #     self.save_checkpoint(epoch, 'best_dead_precision.pt')\n",
    "    \n",
    "    # Early stopping based on primary metric\n",
    "                if metrics['map'] > self.best_map:\n",
    "                    early_stop_counter = 0\n",
    "                    self.best_map = metrics['map']\n",
    "                else:\n",
    "                    early_stop_counter += 1\n",
    "    \n",
    "\n",
    "                print(f\"Epoch {epoch+1} - Train Loss: {epoch_loss/len(train_loader):.4f} | Cls.F1: {metrics['f1']:.2f}\")\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1} - Train Loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "            self._plot_training_curves(self.train_losses, self.val_losses)\n",
    "            self.save_checkpoint(epoch, 'last.pt')\n",
    "\n",
    "        \n",
    "    def validate(self, loader, test_mode=False):\n",
    "        metric = MeanAveragePrecision()\n",
    "        self.model.eval()\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        detection_stats = {\n",
    "        'total_true': 0,\n",
    "        'total_pred': 0,\n",
    "        'correct': 0\n",
    "        }\n",
    "        per_class_metrics = {class_name: {'tp': 0, 'fp': 0, 'fn': 0} \n",
    "                            for class_name in self.class_names}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, targets in loader:\n",
    "                images = [img.to(self.device) for img in images]\n",
    "                targets = [{k: v.to(self.device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "                outputs = self.model(images)\n",
    "\n",
    "                preds = []\n",
    "                for out in outputs:\n",
    "                    preds.append({\n",
    "                        'boxes': out['boxes'],\n",
    "                        'scores': out['scores'], \n",
    "                        'labels': out['labels']\n",
    "                    })\n",
    "                metric.update(preds, targets)\n",
    "            \n",
    "            # Process predictions and calculate matches\n",
    "                for out, tgt in zip(outputs, targets):\n",
    "                    if len(out['boxes']) == 0 or len(tgt['boxes']) == 0:\n",
    "                        continue\n",
    "\n",
    "                    # Inside the validation loop, for each image pair (out, tgt)\n",
    "                    detection_stats['total_pred'] += len(out['boxes'])\n",
    "                    detection_stats['total_true'] += len(tgt['boxes'])\n",
    "                    \n",
    "                    iou_matrix = box_iou(out['boxes'], tgt['boxes'])\n",
    "                    matched_true = [False] * len(tgt['labels'])\n",
    "                \n",
    "                    for pred_idx in range(len(out['boxes'])):\n",
    "                        best_iou = -1\n",
    "                        best_true_idx = -1\n",
    "                    \n",
    "                        for true_idx in range(len(tgt['boxes'])):\n",
    "                            if not matched_true[true_idx] and iou_matrix[pred_idx, true_idx] >= 0.5:\n",
    "                                if iou_matrix[pred_idx, true_idx] > best_iou:\n",
    "                                    best_iou = iou_matrix[pred_idx, true_idx]\n",
    "                                    best_true_idx = true_idx\n",
    "                    \n",
    "                        if best_true_idx != -1:\n",
    "                            matched_true[best_true_idx] = True\n",
    "                            pred_label = out['labels'][pred_idx].item()\n",
    "                            true_label = tgt['labels'][best_true_idx].item()\n",
    "                        \n",
    "                            if pred_label == true_label:\n",
    "                                detection_stats['correct'] += 1\n",
    "                                per_class_metrics[self.class_names[true_label]]['tp'] += 1\n",
    "                            else:\n",
    "                                per_class_metrics[self.class_names[pred_label]]['fp'] += 1\n",
    "                                per_class_metrics[self.class_names[true_label]]['fn'] += 1\n",
    "                        \n",
    "                            all_preds.append(pred_label)\n",
    "                            all_targets.append(true_label)\n",
    "                        else:\n",
    "                            pred_label = out['labels'][pred_idx].item()\n",
    "                            per_class_metrics[self.class_names[pred_label]]['fp'] += 1\n",
    "                \n",
    "                # Count unmatched true boxes as false negatives\n",
    "                    for true_idx, matched in enumerate(matched_true):\n",
    "                        if not matched:\n",
    "                            true_label = tgt['labels'][true_idx].item()\n",
    "                            per_class_metrics[self.class_names[true_label]]['fn'] += 1\n",
    "\n",
    "    # Calculate metrics\n",
    "        precision = detection_stats['correct'] / detection_stats['total_pred'] if detection_stats['total_pred'] > 0 else 0\n",
    "        recall = detection_stats['correct'] / detection_stats['total_true'] if detection_stats['total_true'] > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    # Only calculate confusion matrix in test mode\n",
    "        cm = None\n",
    "        if len(all_preds) > 0:\n",
    "            cls_metrics = {\n",
    "            'precision': precision_score(all_targets, all_preds, average='weighted', zero_division=0),\n",
    "            'recall': recall_score(all_targets, all_preds, average='weighted', zero_division=0),\n",
    "            'f1': f1_score(all_targets, all_preds, average='weighted', zero_division=0)\n",
    "            }\n",
    "            if test_mode:  # Only calculate CM during testing\n",
    "                cm = confusion_matrix(all_targets, all_preds, labels=range(len(self.class_names)))\n",
    "                self._plot_confusion_matrix(cm)\n",
    "                cls_metrics['confusion_matrix'] = cm.tolist()\n",
    "        else:\n",
    "            cls_metrics = {\n",
    "                'precision': 0,\n",
    "                'recall': 0,\n",
    "                'f1': 0,\n",
    "                'confusion_matrix': []\n",
    "            }\n",
    "    \n",
    "    # Calculate per-class metrics\n",
    "        class_wise_results = {}\n",
    "        for class_name in self.class_names:\n",
    "            tp = per_class_metrics[class_name]['tp']\n",
    "            fp = per_class_metrics[class_name]['fp']\n",
    "            fn = per_class_metrics[class_name]['fn']\n",
    "        \n",
    "            class_wise_results[class_name] = {\n",
    "                'precision': tp / (tp + fp) if (tp + fp) > 0 else 0,\n",
    "            'recall': tp / (tp + fn) if (tp + fn) > 0 else 0,\n",
    "                'f1': 2*tp/(2*tp + fp + fn) if (tp + fp + fn) > 0 else 0,\n",
    "            'support': tp + fn\n",
    "            }\n",
    "\n",
    "        combined_metrics = {\n",
    "            'map': metric.compute()['map'].item(),\n",
    "            'detection_precision': precision,\n",
    "            'detection_recall': recall,\n",
    "            'detection_f1': f1,\n",
    "            'per_class_metrics': class_wise_results,  # Keeps your exact key name\n",
    "            **cls_metrics,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "    \n",
    "        self.model.train()\n",
    "        return combined_metrics\n",
    "\n",
    "    def analyze_test_set(self, data_root):\n",
    "        test_dir = Path(data_root) / 'test'\n",
    "        test_set = CoralDataset(\n",
    "        root_dir=test_dir,\n",
    "        transform=self.get_transform(train=False))\n",
    "        test_loader = DataLoader(\n",
    "            test_set, \n",
    "            batch_size=8,\n",
    "            collate_fn=lambda x: tuple(zip(*x)))\n",
    "    \n",
    "        test_metrics = self.validate(test_loader, test_mode=True)\n",
    "\n",
    "        print(\"\\nPer-Class Test Metrics:\")\n",
    "        for class_name, metrics in test_metrics['per_class_metrics'].items():\n",
    "            print(f\"Class: {class_name}\")\n",
    "            print(f\"  Precision: {metrics['precision']:.2%}\")\n",
    "            print(f\"  Recall: {metrics['recall']:.2%}\")\n",
    "            print(f\"  F1 Score: {metrics['f1']:.2%}\")\n",
    "            print(f\"  Support: {metrics['support']}\")\n",
    "        \n",
    "        print(\"\\nFinal Detection Test Metrics:\")\n",
    "        print(f\"Precision: {test_metrics['detection_precision']:.2%}\")\n",
    "        print(f\"Recall: {test_metrics['detection_recall']:.2%}\")\n",
    "        print(f\"F1 Score: {test_metrics['detection_f1']:.2%}\")\n",
    "    \n",
    "        return test_metrics\n",
    "        \n",
    "    def _calculate_metrics(self, true_labels, pred_labels):\n",
    "        # Classification metrics\n",
    "        precision = precision_score(true_labels, pred_labels, average='weighted', zero_division=0)\n",
    "        recall = recall_score(true_labels, pred_labels, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(true_labels, pred_labels, average='weighted', zero_division=0)\n",
    "        cm = confusion_matrix(true_labels, pred_labels)\n",
    "        \n",
    "        self._plot_confusion_matrix(cm)\n",
    "        return {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'confusion_matrix': cm.tolist()\n",
    "        }\n",
    "\n",
    "    def _plot_confusion_matrix(self, cm):\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        try:\n",
    "            sns.heatmap(cm, annot=True, fmt='d',\n",
    "                  xticklabels=self.class_names,\n",
    "                  yticklabels=self.class_names,\n",
    "                  cmap='Blues')\n",
    "            plt.title('Confusion Matrix')\n",
    "            plt.tight_layout()\n",
    "        \n",
    "        # Save to checkpoint directory with high resolution\n",
    "            save_path = self.checkpoint_dir / 'confusion_matrix.png'\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"Saved confusion matrix to: {save_path}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error plotting confusion matrix: {str(e)}\")\n",
    "        finally:\n",
    "            plt.close()\n",
    "    \n",
    "\n",
    "    def _plot_training_curves(self, train_losses, val_losses):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        try:\n",
    "            plt.plot(train_losses, label='Train Loss', linewidth=2)\n",
    "            if val_losses:\n",
    "                plt.plot(val_losses, label='Validation Loss', linewidth=2)\n",
    "            \n",
    "            plt.xlabel('Epochs', fontsize=12)\n",
    "            plt.ylabel('Loss', fontsize=12)\n",
    "            plt.legend(fontsize=12)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.title('Training Curves', fontsize=14)\n",
    "        \n",
    "        # Save to checkpoint directory\n",
    "            save_path = self.checkpoint_dir / 'training_curves.png'\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"Saved training curves to: {save_path}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error plotting training curves: {str(e)}\")\n",
    "        finally:\n",
    "            plt.close()\n",
    "\n",
    "    # def analyze_test_set(self, data_root):\n",
    "    #     test_dir = Path(data_root) / 'test'\n",
    "    #     test_set = CoralDataset(\n",
    "    #         root_dir=test_dir,\n",
    "    #         transform=self.get_transform(train=False))\n",
    "    #     test_loader = DataLoader(\n",
    "    #         test_set, \n",
    "    #         batch_size=8,\n",
    "    #         collate_fn=lambda x: tuple(zip(*x)))\n",
    "    \n",
    "    # # Single return value capture\n",
    "    #     test_metrics = self.validate(test_loader, test_mode=True)\n",
    "\n",
    "    #     print(\"\\nPer-Class Test Metrics:\")\n",
    "    #     for class_name, metrics in test_metrics['per_class_metrics'].items():\n",
    "    #         print(f\"Class: {class_name}\")\n",
    "    #         print(f\"  Precision: {metrics['precision']:.2%}\")\n",
    "    #         print(f\"  Recall: {metrics['recall']:.2%}\")\n",
    "    #         print(f\"  F1 Score: {metrics['f1']:.2%}\")\n",
    "\n",
    "    #     print(\"\\nFinal Detection Test Metrics:\")\n",
    "    #     print(f\"Precision: {test_metrics['detection_precision']:.2%}\")\n",
    "    #     print(f\"Recall: {test_metrics['detection_recall']:.2%}\")\n",
    "    #     print(f\"F1 Score: {test_metrics['detection_f1']:.2%}\")\n",
    "    \n",
    "    # # Optional: Save test metrics separately\n",
    "    #     with open(self.checkpoint_dir / 'test_metrics1.json', 'w') as f:\n",
    "    #         json.dump(test_metrics, f, indent=4)\n",
    "        \n",
    "    #     return test_metrics\n",
    "\n",
    "    def save_checkpoint(self, epoch, filename):\n",
    "        torch.save({\n",
    "            'model': self.model.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'best_map': self.best_map\n",
    "        }, self.checkpoint_dir / filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "b8483444-c9a3-4735-a87d-218540130b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rugved\\AppData\\Local\\Temp\\ipykernel_22508\\1745321145.py:39: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(self.checkpoint_dir / 'last.pt', map_location=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming training from epoch 22\n"
     ]
    }
   ],
   "source": [
    "analyzer = CoralRetinaNetMobile(num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "b853e2af-105d-40c1-b960-7dadbdd05a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batches: 310, Validation batches: 12\n"
     ]
    }
   ],
   "source": [
    "data_root = r\"C:\\Users\\Rugved\\Downloads\\marjan-segmentaion.v15i.voc\"\n",
    "    \n",
    "train_loader, val_loader = analyzer.create_loaders(data_root)\n",
    "print(f\"Training batches: {len(train_loader)}, Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "9ee1aa18-45c3-40b7-90ee-bda632467fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23/24\n",
      "Batch 50/310 - Loss: 0.5097\n",
      "Batch 100/310 - Loss: 0.7898\n",
      "Batch 150/310 - Loss: 0.5264\n",
      "Batch 200/310 - Loss: 0.4766\n",
      "Batch 250/310 - Loss: 0.5872\n",
      "Batch 300/310 - Loss: 0.4943\n",
      "Epoch 23 - Train Loss: 0.6034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rugved\\anaconda3\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MeanAveragePrecision was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 - Train Loss: 0.6034 | Cls.F1: 0.49\n",
      "Saved training curves to: checkpoints\\training_curves.png\n",
      "\n",
      "Epoch 24/24\n",
      "Batch 50/310 - Loss: 0.5762\n",
      "Batch 100/310 - Loss: 0.3928\n",
      "Batch 150/310 - Loss: 0.6090\n",
      "Batch 200/310 - Loss: 0.6087\n",
      "Batch 250/310 - Loss: 0.5474\n",
      "Batch 300/310 - Loss: 0.4911\n",
      "Epoch 24 - Train Loss: 0.5443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rugved\\anaconda3\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MeanAveragePrecision was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 - Train Loss: 0.5443 | Cls.F1: 0.54\n",
      "Saved training curves to: checkpoints\\training_curves.png\n"
     ]
    }
   ],
   "source": [
    "analyzer.train(train_loader, epochs=24, val_loader=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "d499b3d3-182d-48d0-8536-98786b73610b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved confusion matrix to: checkpoints\\confusion_matrix.png\n",
      "\n",
      "Per-Class Test Metrics:\n",
      "Class: Bleached\n",
      "  Precision: 3.60%\n",
      "  Recall: 3.44%\n",
      "  F1 Score: 3.52%\n",
      "  Support: 349\n",
      "Class: Dead\n",
      "  Precision: 5.26%\n",
      "  Recall: 5.75%\n",
      "  F1 Score: 5.50%\n",
      "  Support: 313\n",
      "Class: Healthy\n",
      "  Precision: 6.03%\n",
      "  Recall: 6.91%\n",
      "  F1 Score: 6.44%\n",
      "  Support: 304\n",
      "\n",
      "Final Detection Test Metrics:\n",
      "Precision: 4.99%\n",
      "Recall: 5.28%\n",
      "F1 Score: 5.13%\n",
      "\n",
      "Final Test Metrics:\n",
      "Precision: 67.88%\n",
      "Recall: 56.04%\n",
      "F1 Score: 56.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rugved\\anaconda3\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: The ``compute`` method of metric MeanAveragePrecision was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "test_metrics = analyzer.analyze_test_set(data_root)\n",
    "print(\"\\nFinal Test Metrics:\")\n",
    "print(f\"Precision: {test_metrics['precision']:.2%}\")\n",
    "print(f\"Recall: {test_metrics['recall']:.2%}\")\n",
    "print(f\"F1 Score: {test_metrics['f1']:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "6e9cb7f8-ae54-4196-957d-7b4e79f4de65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'map': -1.0, 'detection_precision': 0.04985337243401759, 'detection_recall': 0.052795031055900624, 'detection_f1': 0.05128205128205129, 'per_class_metrics': {'Bleached': {'precision': 0.036036036036036036, 'recall': 0.034383954154727794, 'f1': 0.03519061583577713, 'support': 349}, 'Dead': {'precision': 0.05263157894736842, 'recall': 0.05750798722044728, 'f1': 0.0549618320610687, 'support': 313}, 'Healthy': {'precision': 0.0603448275862069, 'recall': 0.06907894736842106, 'f1': 0.06441717791411043, 'support': 304}}, 'precision': 0.6788323457104176, 'recall': 0.5604395604395604, 'f1': 0.5698979591836734, 'confusion_matrix': [[12, 0, 13], [2, 18, 19], [3, 3, 21]], 'timestamp': '2025-03-30T19:44:09.985825'}\n"
     ]
    }
   ],
   "source": [
    "print(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7c5b779-6302-44bd-9cd9-9558b0483f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CoralRetinaNetMobile:   #vlast\n",
    "#     def __init__(self, num_classes=3, pretrained=True):\n",
    "#         self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#         self.class_names = ['Bleached', 'Dead', 'Healthy']\n",
    "#         self.checkpoint_dir = Path('checkpoints')\n",
    "#         self.checkpoint_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "#         # Model setup\n",
    "#         backbone = mobilenet_v3_small(weights='DEFAULT' if pretrained else None).features\n",
    "#         backbone.out_channels = 576\n",
    "        \n",
    "#         anchor_generator = AnchorGenerator(\n",
    "#             sizes=((16, 32, 64, 128),),\n",
    "#             aspect_ratios=((0.5, 1.0, 2.0),) * 4\n",
    "#         )\n",
    "        \n",
    "#         self.model = RetinaNet(\n",
    "#             backbone,\n",
    "#             num_classes=num_classes,\n",
    "#             anchor_generator=anchor_generator,\n",
    "#             box_score_thresh=0.25\n",
    "#         ).to(self.device)\n",
    "        \n",
    "#         # Checkpoint loading\n",
    "#         if (self.checkpoint_dir / 'last.pt').exists():\n",
    "#             self.load_checkpoint()\n",
    "#         else:\n",
    "#             self.optimizer = torch.optim.RAdam(self.model.parameters(), lr=3e-4)\n",
    "#             self.scaler = torch.amp.GradScaler()\n",
    "#             self.start_epoch = 0\n",
    "#             self.best_map = 0.0  # Track mAP instead of loss\n",
    "\n",
    "#     def load_checkpoint(self):\n",
    "#         checkpoint = torch.load(self.checkpoint_dir / 'last.pt', map_location=self.device)\n",
    "#         self.model.load_state_dict(checkpoint['model'])\n",
    "#         self.optimizer = torch.optim.RAdam(self.model.parameters())\n",
    "#         self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "#         self.start_epoch = checkpoint['epoch'] + 1\n",
    "#         self.best_map = checkpoint.get('best_map', 0.0)\n",
    "#         self.scaler = torch.amp.GradScaler()\n",
    "#         print(f\"Resuming training from epoch {self.start_epoch}\")\n",
    "\n",
    "#     def get_transform(self, train=True):\n",
    "#         return transforms.Compose([\n",
    "#             transforms.ToImage(),\n",
    "#             transforms.ToDtype(torch.float32, scale=True),\n",
    "#             transforms.Resize((320, 320), antialias=True),\n",
    "#             transforms.RandomHorizontalFlip(p=0.3) if train else transforms.Identity(),\n",
    "#         ])\n",
    "\n",
    "#     def create_loaders(self, data_root):\n",
    "#         train_loader = DataLoader(\n",
    "#             CoralDataset(Path(data_root)/'train', self.get_transform(True)),\n",
    "#             batch_size=16,\n",
    "#             shuffle=True,\n",
    "#             num_workers=0,  # Required for Windows stability\n",
    "#             pin_memory=True,\n",
    "#             collate_fn=lambda x: tuple(zip(*x))\n",
    "#         )\n",
    "#         val_loader = DataLoader(\n",
    "#             CoralDataset(Path(data_root)/'valid', self.get_transform(False)),\n",
    "#             batch_size=32,\n",
    "#             num_workers=0,\n",
    "#             pin_memory=True,\n",
    "#             collate_fn=lambda x: tuple(zip(*x))\n",
    "#         )\n",
    "#         return train_loader, val_loader\n",
    "\n",
    "#     def train(self, train_loader, epochs=80, val_loader=None, early_stop_patience=12):\n",
    "#         self.model.train()\n",
    "#         torch.backends.cudnn.benchmark = True\n",
    "#         early_stop_counter = 0\n",
    "\n",
    "#         for epoch in range(self.start_epoch, epochs):\n",
    "#             print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "#             epoch_loss = 0.0\n",
    "\n",
    "#         # Training phase\n",
    "#             for i, (images, targets) in enumerate(train_loader):\n",
    "#                 images = [img.to(self.device) for img in images]\n",
    "#                 targets = [{k: v.to(self.device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "#                 with torch.amp.autocast(device_type='cuda', enabled=self.device.type == 'cuda'):\n",
    "#                     loss_dict = self.model(images, targets)\n",
    "#                     losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "#                 self.optimizer.zero_grad()\n",
    "#                 self.scaler.scale(losses).backward()\n",
    "#                 self.scaler.step(self.optimizer)\n",
    "#                 self.scaler.update()\n",
    "#                 epoch_loss += losses.item()\n",
    "\n",
    "#             # Log batch loss every 30 batches\n",
    "#                 if (i + 1) % 30 == 0:\n",
    "#                     print(f\"Batch {i+1}/{len(train_loader)} - Loss: {losses.item():.4f}\")\n",
    "\n",
    "#         # Validation phase\n",
    "#             if val_loader:\n",
    "#                 map_score, metrics = self.validate(val_loader)\n",
    "            \n",
    "#                 if map_score > self.best_map:\n",
    "#                     self.best_map = map_score\n",
    "#                     self.save_checkpoint(epoch, 'best.pt')\n",
    "#                     early_stop_counter = 0\n",
    "#                 else:\n",
    "#                     early_stop_counter += 1\n",
    "\n",
    "#                 if early_stop_counter >= early_stop_patience:\n",
    "#                     print(f\"Early stopping at epoch {epoch+1}\")\n",
    "#                     break\n",
    "\n",
    "#             self.save_checkpoint(epoch, 'last.pt')\n",
    "#             print(f\"Epoch {epoch+1} - Train Loss: {epoch_loss/len(train_loader):.4f} | mAP: {map_score:.4f}\")\n",
    "        \n",
    "#     def validate(self, loader):\n",
    "#         metric = MeanAveragePrecision()\n",
    "#         self.model.eval()\n",
    "#         all_preds = []\n",
    "#         all_targets = []\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             for images, targets in loader:\n",
    "#                 images = [img.to(self.device) for img in images]\n",
    "#                 targets = [{k: v.to(self.device) for k, v in t.items()} for t in targets]\n",
    "                \n",
    "#                 outputs = self.model(images)\n",
    "                \n",
    "#                 # For detection metrics\n",
    "#                 metric.update(\n",
    "#                     [{'boxes': o['boxes'], 'scores': o['scores'], 'labels': o['labels']} for o in outputs],\n",
    "#                     [{'boxes': t['boxes'], 'labels': t['labels']} for t in targets]\n",
    "#                 )\n",
    "                \n",
    "#                 # For classification-style metrics\n",
    "#                 for out, tgt in zip(outputs, targets):\n",
    "#                     if len(out['labels']) > 0:\n",
    "#                         all_preds.extend(out['labels'].cpu().numpy())\n",
    "#                         all_targets.extend(tgt['labels'].cpu().numpy())\n",
    "\n",
    "#         # Detection metrics\n",
    "#         detection_metrics = metric.compute()\n",
    "        \n",
    "#         # Classification-style metrics\n",
    "#         if len(all_preds) > 0 and len(all_targets) > 0:\n",
    "#             cls_metrics = self._calculate_metrics(np.array(all_targets), np.array(all_preds))\n",
    "#         else:\n",
    "#             cls_metrics = {}\n",
    "        \n",
    "#         # Combine metrics\n",
    "#         combined_metrics = {\n",
    "#             **detection_metrics,\n",
    "#             **cls_metrics,\n",
    "#             'timestamp': datetime.now().isoformat()\n",
    "#         }\n",
    "        \n",
    "#         with open('metrics.json', 'w') as f:\n",
    "#             json.dump(combined_metrics, f)\n",
    "            \n",
    "#         return detection_metrics['map'], combined_metrics\n",
    "\n",
    "#     def _calculate_metrics(self, true_labels, pred_labels):\n",
    "#         # Classification metrics\n",
    "#         precision = precision_score(true_labels, pred_labels, \n",
    "#                                   average='weighted', zero_division=0)\n",
    "#         recall = recall_score(true_labels, pred_labels,\n",
    "#                             average='weighted', zero_division=0)\n",
    "#         f1 = f1_score(true_labels, pred_labels,\n",
    "#                     average='weighted', zero_division=0)\n",
    "#         cm = confusion_matrix(true_labels, pred_labels)\n",
    "        \n",
    "#         # Plot confusion matrix\n",
    "#         self._plot_confusion_matrix(cm)\n",
    "        \n",
    "#         return {\n",
    "#             'precision': precision,\n",
    "#             'recall': recall,\n",
    "#             'f1': f1,\n",
    "#             'confusion_matrix': cm.tolist()\n",
    "#         }\n",
    "\n",
    "#     def _plot_confusion_matrix(self, cm):\n",
    "#         plt.figure(figsize=(10,8))\n",
    "#         sns.heatmap(cm, annot=True, fmt='d', \n",
    "#                   xticklabels=self.class_names,\n",
    "#                   yticklabels=self.class_names)\n",
    "#         plt.title('Confusion Matrix')\n",
    "#         plt.savefig('confusion_matrix.png')\n",
    "#         plt.close()\n",
    "\n",
    "#     def _plot_training_curves(self, train_losses, val_losses):\n",
    "#         plt.figure()\n",
    "#         plt.plot(train_losses, label='Train Loss')\n",
    "#         if val_losses:\n",
    "#             plt.plot(val_losses, label='Validation Loss')\n",
    "#         plt.xlabel('Epochs')\n",
    "#         plt.ylabel('Loss')\n",
    "#         plt.legend()\n",
    "#         plt.title('Training Curves')\n",
    "#         plt.savefig('training_curves.png')\n",
    "#         plt.close()\n",
    "\n",
    "#     def analyze_test_set(self, data_root):\n",
    "#         test_dir = Path(data_root) / 'test'\n",
    "#         test_set = CoralDataset(\n",
    "#             root_dir=test_dir,\n",
    "#             transform=self.get_transform(train=False)\n",
    "#         )\n",
    "#         test_loader = DataLoader(\n",
    "#             test_set, batch_size=8,\n",
    "#             collate_fn=lambda x: tuple(zip(*x))\n",
    "#         )\n",
    "#         _, metrics = self.validate(test_loader)\n",
    "#         return metrics\n",
    "\n",
    "#     def save_checkpoint(self, epoch, filename):\n",
    "#         torch.save({\n",
    "#             'model': self.model.state_dict(),\n",
    "#             'optimizer': self.optimizer.state_dict(),\n",
    "#             'epoch': epoch,\n",
    "#             'best_map': self.best_map\n",
    "#         }, self.checkpoint_dir / filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fc875d6-dee3-4854-95de-1e56cbeb41c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# class CoralRetinaNetMobile:\n",
    "#     def __init__(self, num_classes=3, pretrained=True):\n",
    "#         self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#         self.class_names = ['Bleached', 'Dead', 'Healthy']\n",
    "#         self.checkpoint_dir = Path('checkpoints')\n",
    "#         self.checkpoint_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "#         # Load MobileNetV3-small backbone\n",
    "#         backbone = mobilenet_v3_small(weights='DEFAULT' if pretrained else None).features\n",
    "#         backbone.out_channels = 576  # Critical for MobileNetV3-small\n",
    "        \n",
    "#         # Custom anchor generator for coral sizes\n",
    "#         anchor_generator = AnchorGenerator(\n",
    "#             sizes=((16, 32, 64, 128),),\n",
    "#             aspect_ratios=((0.5, 1.0, 2.0),) * 4\n",
    "#         )\n",
    "        \n",
    "#         # Initialize model\n",
    "#         self.model = RetinaNet(\n",
    "#             backbone,\n",
    "#             num_classes=num_classes,\n",
    "#             anchor_generator=anchor_generator,\n",
    "#             box_score_thresh=0.25\n",
    "#         ).to(self.device)\n",
    "        \n",
    "#         # Load checkpoint if exists\n",
    "#         if (self.checkpoint_dir / 'last.pt').exists():\n",
    "#             self.load_checkpoint()\n",
    "#         else:\n",
    "#             self.optimizer = torch.optim.RAdam(self.model.parameters(), lr=3e-4)\n",
    "#             self.scaler = self.create_scaler()\n",
    "#             self.start_epoch = 0\n",
    "#             self.best_map = 0.0  # Track mAP instead of loss\n",
    "\n",
    "#     def create_scaler(self):\n",
    "#         return torch.amp.GradScaler('cuda', enabled=self.device.type == 'cuda')\n",
    "\n",
    "#     def load_checkpoint(self):\n",
    "#         checkpoint = torch.load(self.checkpoint_dir / 'last.pt', map_location=self.device)\n",
    "#         self.model.load_state_dict(checkpoint['model'])\n",
    "#         self.optimizer = torch.optim.RAdam(self.model.parameters())\n",
    "#         self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "#         self.start_epoch = checkpoint['epoch'] + 1\n",
    "#         self.best_loss = checkpoint['best_loss']\n",
    "#         self.scaler = self.create_scaler()\n",
    "#         print(f\"Resuming training from epoch {self.start_epoch}\")\n",
    "\n",
    "#     def get_transform(self, train=True):\n",
    "#         return transforms.Compose([\n",
    "#             transforms.ToImage(),\n",
    "#             transforms.ToDtype(torch.float32, scale=True),\n",
    "#             transforms.Resize((320, 320), antialias=True),\n",
    "#             transforms.RandomHorizontalFlip(p=0.3) if train else transforms.Identity(),\n",
    "#             transforms.ColorJitter(brightness=0.15, contrast=0.15) if train else transforms.Identity(),\n",
    "#         ])\n",
    "\n",
    "#     def create_loaders(self, data_root):\n",
    "#         train_loader = DataLoader(\n",
    "#             CoralDataset(Path(data_root)/'train', self.get_transform(True)),\n",
    "#             batch_size=16,\n",
    "#             shuffle=True,\n",
    "#             num_workers=0,\n",
    "#             pin_memory=True,\n",
    "#             collate_fn=lambda x: tuple(zip(*x))\n",
    "#         )\n",
    "#         val_loader = DataLoader(\n",
    "#             CoralDataset(Path(data_root)/'valid', self.get_transform(False)),\n",
    "#             batch_size=32,\n",
    "#             num_workers=0,\n",
    "#             pin_memory=True,\n",
    "#             collate_fn=lambda x: tuple(zip(*x))\n",
    "#         )\n",
    "#         return train_loader, val_loader\n",
    "\n",
    "#     def train(self, train_loader, epochs=80, val_loader=None, early_stop_patience=12):\n",
    "#         self.model.train()\n",
    "#         torch.backends.cudnn.benchmark = True\n",
    "#         train_losses, val_losses = [], []\n",
    "#         early_stop_counter = 0\n",
    "\n",
    "#         for epoch in range(self.start_epoch, epochs):\n",
    "#             print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "#             epoch_loss = 0.0\n",
    "\n",
    "#             for i, (images, targets) in enumerate(train_loader):\n",
    "#                 print(f\"Processing first batch (size: {len(images)})\") if i == 0 else None\n",
    "\n",
    "#                 try:\n",
    "#                 # Move data to the device\n",
    "#                     images = [img.to(self.device) for img in images]\n",
    "#                     targets = [{k: v.to(self.device) for k, v in t.items()} for t in targets]\n",
    "#                     if i == 0:\n",
    "#                         print(f\"  Batch {i+1}: Images moved to device\")\n",
    "                \n",
    "#                 # Forward pass with automatic mixed precision\n",
    "#                     with torch.cuda.amp.autocast(enabled=self.device.type == 'cuda'):\n",
    "#                         if i == 0:\n",
    "#                             print(f\"  Batch {i+1}: Starting forward pass\")\n",
    "#                         loss_dict = self.model(images, targets)\n",
    "#                         if i == 0:\n",
    "#                             print(f\"  Batch {i+1}: Forward pass complete, computing loss\")\n",
    "#                         losses = sum(loss for loss in loss_dict.values())\n",
    "#                         if i == 0:\n",
    "#                             print(f\"  Batch {i+1}: Loss computed: {losses.item():.4f}\")\n",
    "                \n",
    "#                 # Zero gradients, backward pass, and step\n",
    "#                     self.optimizer.zero_grad()\n",
    "#                     if i == 0:\n",
    "#                         print(f\"  Batch {i+1}: Starting backward pass\")\n",
    "#                     self.scaler.scale(losses).backward()\n",
    "#                     if i == 0:\n",
    "#                         print(f\"  Batch {i+1}: Backward pass complete\")\n",
    "#                     self.scaler.step(self.optimizer)\n",
    "#                     self.scaler.update()\n",
    "\n",
    "#                     epoch_loss += losses.item()\n",
    "\n",
    "#                     if i % 20 == 0:\n",
    "#                         print(f\"  Batch {i+1}/{len(train_loader)} processed, current loss: {losses.item():.4f}\")\n",
    "\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error in batch {i+1}: {str(e)}\")\n",
    "#                     print(f\"Image shapes: {[img.shape for img in images]}\")\n",
    "#                     print(f\"Target keys: {[list(t.keys()) for t in targets]}\")\n",
    "#                     raise e\n",
    "\n",
    "#             avg_train_loss = epoch_loss / len(train_loader)\n",
    "#             train_losses.append(avg_train_loss)\n",
    "#             print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "#         # Validation\n",
    "#             if val_loader:\n",
    "#                 val_loss, metrics = self.validate(val_loader)\n",
    "#                 val_losses.append(val_loss)\n",
    "\n",
    "#                 if val_loss < self.best_loss:\n",
    "#                     self.best_loss = val_loss\n",
    "#                     self.save_checkpoint(epoch, 'best.pt')\n",
    "#                     early_stop_counter = 0\n",
    "#                 else:\n",
    "#                     early_stop_counter += 1\n",
    "\n",
    "#                 if early_stop_counter >= early_stop_patience:\n",
    "#                     print(f\"Early stopping at epoch {epoch+1}\")\n",
    "#                     break\n",
    "\n",
    "#             self.save_checkpoint(epoch, 'last.pt')\n",
    "\n",
    "#         self._plot_training_curves(train_losses, val_losses)\n",
    "#         return train_losses, val_losses\n",
    "\n",
    "\n",
    "#     def save_checkpoint(self, epoch, filename):\n",
    "#         torch.save({\n",
    "#             'model': self.model.state_dict(),\n",
    "#             'optimizer': self.optimizer.state_dict(),\n",
    "#             'epoch': epoch,\n",
    "#             'best_loss': self.best_loss\n",
    "#         }, self.checkpoint_dir / filename)\n",
    "\n",
    "#     def validate(self, loader):\n",
    "#         self.model.eval()\n",
    "#         total_loss = 0.0\n",
    "#         all_preds, all_targets = [], []\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             for images, targets in loader:\n",
    "#                 images = [img.to(self.device) for img in images]\n",
    "#                 targets = [{k: v.to(self.device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "#             # Case 1: Get losses (requires gradient)\n",
    "#                 with torch.amp.autocast(device_type='cuda', enabled=self.device.type == 'cuda'):\n",
    "#                     loss_dict = self.model(images, targets)  # Only works in train mode\n",
    "                \n",
    "#                 # Handle both dict and list outputs\n",
    "#                     if isinstance(loss_dict, dict):\n",
    "#                         total_loss += sum(loss for loss in loss_dict.values()).item()\n",
    "#                     else:\n",
    "#                     # If no losses returned (eval mode), skip loss calculation\n",
    "#                         total_loss = float('nan')\n",
    "            \n",
    "#             # Case 2: Get predictions\n",
    "#                 outputs = self.model(images)  # Forward pass without targets\n",
    "#                 for out, tgt in zip(outputs, targets):\n",
    "#                     all_preds.extend(out['labels'].cpu().numpy())\n",
    "#                     all_targets.extend(tgt['labels'].cpu().numpy())\n",
    "\n",
    "#         avg_loss = total_loss if not isinstance(total_loss, float) else 0.0\n",
    "#         metrics = self._calculate_metrics(np.array(all_targets), np.array(all_preds))\n",
    "#         return avg_loss, metrics\n",
    "\n",
    "#     # Keep _calculate_metrics, _plot_training_curves, and analyze_test_set \n",
    "#     # implementations from previous code (they remain the same)\n",
    "\n",
    "#     def _calculate_metrics(self, true_labels, pred_labels):\n",
    "#         # Classification metrics\n",
    "#         precision = precision_score(true_labels, pred_labels, average='weighted', zero_division=0)\n",
    "#         recall = recall_score(true_labels, pred_labels, average='weighted', zero_division=0)\n",
    "#         f1 = f1_score(true_labels, pred_labels, average='weighted', zero_division=0)\n",
    "#         cm = confusion_matrix(true_labels, pred_labels)\n",
    "        \n",
    "#         # Plot confusion matrix\n",
    "#         plt.figure(figsize=(10,8))\n",
    "#         sns.heatmap(cm, annot=True, fmt='d', xticklabels=self.class_names, yticklabels=self.class_names)\n",
    "#         plt.title('Confusion Matrix')\n",
    "#         plt.savefig('confusion_matrix.png')\n",
    "#         plt.close()\n",
    "        \n",
    "#         # Save metrics\n",
    "#         metrics = {\n",
    "#             'precision': precision,\n",
    "#             'recall': recall,\n",
    "#             'f1': f1,\n",
    "#             'confusion_matrix': cm.tolist(),\n",
    "#             'timestamp': datetime.now().isoformat()\n",
    "#         }\n",
    "        \n",
    "#         with open('metrics.json', 'w') as f:\n",
    "#             json.dump(metrics, f)\n",
    "        \n",
    "#         print(f\"Validation Metrics - Precision: {precision:.2%}, Recall: {recall:.2%}, F1: {f1:.2%}\")\n",
    "#         return metrics\n",
    "\n",
    "#     def _plot_training_curves(self, train_losses, val_losses):\n",
    "#         plt.figure()\n",
    "#         plt.plot(train_losses, label='Train Loss')\n",
    "#         if val_losses:\n",
    "#             plt.plot(val_losses, label='Validation Loss')\n",
    "#         plt.xlabel('Epochs')\n",
    "#         plt.ylabel('Loss')\n",
    "#         plt.legend()\n",
    "#         plt.title('Training Curves')\n",
    "#         plt.savefig('training_curves.png')\n",
    "#         plt.close()\n",
    "\n",
    "#     def analyze_test_set(self, data_root):\n",
    "#         test_dir = Path(data_root) / 'test'\n",
    "#         test_set = CoralDataset(\n",
    "#             root_dir=test_dir,\n",
    "#             transform=self.get_transform(train=False)\n",
    "#         )\n",
    "#         test_loader = DataLoader(\n",
    "#             test_set, batch_size=8,\n",
    "#             collate_fn=lambda x: tuple(zip(*x))\n",
    "#         )\n",
    "#         _, metrics = self.validate(test_loader)\n",
    "#         return metrics\n",
    "\n",
    "\n",
    "# # # Usage example\n",
    "# # if __name__ == \"__main__\":\n",
    "# #     analyzer = CoralRetinaNetMobile(num_classes=3)\n",
    "# #     data_root = r\"C:\\Users\\Rugved\\Downloads\\marjan-segmentaion.v15i.voc\"\n",
    "    \n",
    "# #     train_loader, val_loader = analyzer.create_loaders(data_root)\n",
    "# #     print(f\"Training batches: {len(train_loader)}, Validation batches: {len(val_loader)}\")\n",
    "    \n",
    "# #     analyzer.train(train_loader, epochs=80, val_loader=val_loader)\n",
    "    \n",
    "# #     test_metrics = analyzer.analyze_test_set(Path(data_root)/'test')\n",
    "# #     print(\"\\nFinal Test Metrics:\")\n",
    "# #     print(f\"Precision: {test_metrics['precision']:.2%}\")\n",
    "# #     print(f\"Recall: {test_metrics['recall']:.2%}\")\n",
    "# #     print(f\"F1 Score: {test_metrics['f1']:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da3fa73-21ae-4828-a943-03a274aa188f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(self, loader, test_mode=False):\n",
    "    metric = MeanAveragePrecision()\n",
    "    self.model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    detection_stats = {\n",
    "        'total_true': 0,\n",
    "        'total_pred': 0,\n",
    "        'correct': 0\n",
    "    }\n",
    "    per_class_metrics = {class_name: {'tp': 0, 'fp': 0, 'fn': 0} \n",
    "                        for class_name in self.class_names}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in loader:\n",
    "            images = [img.to(self.device) for img in images]\n",
    "            targets = [{k: v.to(self.device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            outputs = self.model(images)\n",
    "            \n",
    "            # Process predictions and calculate matches\n",
    "            for out, tgt in zip(outputs, targets):\n",
    "                if len(out['boxes']) == 0 or len(tgt['boxes']) == 0:\n",
    "                    continue\n",
    "                    \n",
    "                iou_matrix = box_iou(out['boxes'], tgt['boxes'])\n",
    "                matched_true = [False] * len(tgt['labels'])\n",
    "                \n",
    "                for pred_idx in range(len(out['boxes'])):\n",
    "                    best_iou = -1\n",
    "                    best_true_idx = -1\n",
    "                    \n",
    "                    for true_idx in range(len(tgt['boxes'])):\n",
    "                        if not matched_true[true_idx] and iou_matrix[pred_idx, true_idx] >= 0.5:\n",
    "                            if iou_matrix[pred_idx, true_idx] > best_iou:\n",
    "                                best_iou = iou_matrix[pred_idx, true_idx]\n",
    "                                best_true_idx = true_idx\n",
    "                    \n",
    "                    if best_true_idx != -1:\n",
    "                        matched_true[best_true_idx] = True\n",
    "                        pred_label = out['labels'][pred_idx].item()\n",
    "                        true_label = tgt['labels'][best_true_idx].item()\n",
    "                        \n",
    "                        if pred_label == true_label:\n",
    "                            detection_stats['correct'] += 1\n",
    "                            per_class_metrics[self.class_names[true_label]]['tp'] += 1\n",
    "                        else:\n",
    "                            per_class_metrics[self.class_names[pred_label]]['fp'] += 1\n",
    "                            per_class_metrics[self.class_names[true_label]]['fn'] += 1\n",
    "                        \n",
    "                        all_preds.append(pred_label)\n",
    "                        all_targets.append(true_label)\n",
    "                    else:\n",
    "                        pred_label = out['labels'][pred_idx].item()\n",
    "                        per_class_metrics[self.class_names[pred_label]]['fp'] += 1\n",
    "                \n",
    "                # Count unmatched true boxes as false negatives\n",
    "                for true_idx, matched in enumerate(matched_true):\n",
    "                    if not matched:\n",
    "                        true_label = tgt['labels'][true_idx].item()\n",
    "                        per_class_metrics[self.class_names[true_label]]['fn'] += 1\n",
    "\n",
    "    # Calculate metrics\n",
    "    precision = detection_stats['correct'] / detection_stats['total_pred'] if detection_stats['total_pred'] > 0 else 0\n",
    "    recall = detection_stats['correct'] / detection_stats['total_true'] if detection_stats['total_true'] > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    # Only calculate confusion matrix in test mode\n",
    "    cm = None\n",
    "    if test_mode and len(all_preds) > 0:\n",
    "        cm = confusion_matrix(all_targets, all_preds, labels=range(len(self.class_names)))\n",
    "        self._plot_confusion_matrix(cm)\n",
    "    \n",
    "    # Calculate per-class metrics\n",
    "    class_wise_results = {}\n",
    "    for class_name in self.class_names:\n",
    "        tp = per_class_metrics[class_name]['tp']\n",
    "        fp = per_class_metrics[class_name]['fp']\n",
    "        fn = per_class_metrics[class_name]['fn']\n",
    "        \n",
    "        class_wise_results[class_name] = {\n",
    "            'precision': tp / (tp + fp) if (tp + fp) > 0 else 0,\n",
    "            'recall': tp / (tp + fn) if (tp + fn) > 0 else 0,\n",
    "            'f1': 2*tp/(2*tp + fp + fn) if (tp + fp + fn) > 0 else 0,\n",
    "            'support': tp + fn\n",
    "        }\n",
    "\n",
    "    combined_metrics = {\n",
    "        'map': metric.compute()['map'].item(),\n",
    "        'detection_precision': precision,\n",
    "        'detection_recall': recall,\n",
    "        'detection_f1': f1,\n",
    "        'per_class_metrics': class_wise_results,  # Keeps your exact key name\n",
    "        'confusion_matrix': cm.tolist() if cm is not None else [],\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    self.model.train()\n",
    "    return combined_metrics\n",
    "\n",
    "def analyze_test_set(self, data_root):\n",
    "    test_dir = Path(data_root) / 'test'\n",
    "    test_set = CoralDataset(\n",
    "        root_dir=test_dir,\n",
    "        transform=self.get_transform(train=False))\n",
    "    test_loader = DataLoader(\n",
    "        test_set, \n",
    "        batch_size=8,\n",
    "        collate_fn=lambda x: tuple(zip(*x)))\n",
    "    \n",
    "    test_metrics = self.validate(test_loader, test_mode=True)\n",
    "\n",
    "    print(\"\\nPer-Class Test Metrics:\")\n",
    "    for class_name, metrics in test_metrics['per_class_metrics'].items():\n",
    "        print(f\"Class: {class_name}\")\n",
    "        print(f\"  Precision: {metrics['precision']:.2%}\")\n",
    "        print(f\"  Recall: {metrics['recall']:.2%}\")\n",
    "        print(f\"  F1 Score: {metrics['f1']:.2%}\")\n",
    "        print(f\"  Support: {metrics['support']}\")\n",
    "\n",
    "    print(\"\\nFinal Detection Test Metrics:\")\n",
    "    print(f\"Precision: {test_metrics['detection_precision']:.2%}\")\n",
    "    print(f\"Recall: {test_metrics['detection_recall']:.2%}\")\n",
    "    print(f\"F1 Score: {test_metrics['detection_f1']:.2%}\")\n",
    "    \n",
    "    return test_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
